---
layout: post
title: 损失函数
date: 2017-01-08
categories: jekyll update
---
## 定义

对于分类问题，假设我们有数据集 $$D=\{(\vec x_1, \vec y_1), (\vec x_2, \vec y_2), ..., (\vec x_N, \vec y_N)\}$$，以及模型 $$F$$，其中 $$x_i, y_i$$　均为向量，$$\vec y_i$$ 满足 one-hot 形式；为了量化模型在数据集$$D$$上的表现能力，我们通过定义损失函数(loss function)来计算该量化的值，假设模型的输出为 $$\vec f = F(\vec x)$$，我们有如下两种损失函数：

* 平方和误差

 $$L = \frac{1}{N}\sum_{n=1}^N ||\vec y_n - \vec f_n||^2$$

 可以看出上式中使用向量 $$\vec y$$ 和 $$\vec f$$ 的欧式距离来衡量模型表现能力，如果 $$\vec f_n$$ 越接近目标向量 $$\vec y_n$$ （$$\vec f \rightarrow \vec y$$），那么损失函数 $$L$$ 的值越小，反之越大。因此可以通过最小化 $$L$$ 来提高模型的表现能力。

* 交叉熵

 $$L = －\frac{1}{N}\sum_{n=1}^N \{\vec y_n ln\: \vec f_n + (1-\vec y_n) ln\: (1-\vec f_n)\}$$

 因为向量 $$\vec y_n$$ 是 [one-hot][one-hot-wiki] 形式，因此向量中的元素值 $$y_n^i\in \{0, 1\}$$；而向量 $$\vec f$$ 中的元素值 $$f_n^i\in [0, 1]$$。那么当

 　1. $$y_n^i = 0$$ 时，　

 　上式中等式右侧第一项　$$ y_n^i\: ln \:f_n^i = 0$$，第二项　$$(1-y_n^i)\: ln\: (1 - f_n^i)＝ln\: (1 - f_n^i)$$

 　的值随着　$$f_n^i \rightarrow 0$$　也不断接近 $$0$$。

 　２. $$y_n^i = １$$ 时，　

 　上式中等式右侧第一项　$$ y_n^i\: ln \:f_n^i = ln\: f_n^i$$　的值随着　$$f_n^i \rightarrow 1$$　不断接近　$$0$$；

  　第二项$$(1-y_n^i)\: ln\: (1 - f_n^i)＝0$$。

因此交叉熵形式的损失函数在　$$\vec f \rightarrow \vec y$$　的过程中不断接近 0，同`平方和误差`一样可以用来衡量模型在数据集上的表现能力，并且等式右侧的负号保证 $$L\ge 0$$。


## 例子

现在我们考虑二分类问题 (类别 $$c_0$$ 和 $$c_1$$) 的交叉熵。这时向量 $$\vec y$$ 是二维向量， $$\vec y[0]$$ 和 $$\vec y[1]$$ 分别表示 $$\vec x$$ 属于 $$c_0$$ 和 $$c_1$$ 的概率。为了简化问题，我们假设数据集 $$D$$ 中只包含 $$c_0$$ 类别的样本，这时:

$$y_n[0] \equiv 1, \quad y_n[1]\equiv 0 \qquad n=1, 2, ..., N$$

另外，我们在区间 $$[0,\: 1]$$ 上进行 beta 分布随机采样来代表样本属于 $$c_0$$ 类别的概率。

下图展示了参数 $$(a, b)$$ 在取不同值的情况下 beta 函数的曲线图，可以看出在固定 $$b$$ 的条件下，随着 $$a$$ 值不断增加，曲线不断向右偏斜，即采样数据的值有更大的概率偏向 1，也更接近真实数据 $$y$$。
![beta_function](/images/beta_function.png)

生成数据的代码如下：

{% highlight python %}
def gen_data(N, a, b):
  """
  N: 数据集中的样本数量
  a, b: beta 分布中的参数
  """
  #　样本的真实标签数据
  y = np.zeros(shape=(N, 2))
  y[:, 0] = 1.

  #　模型预测出的概率值
  f_0 = np.random.beta(a, b, size=(N, 1))
  f_1 = 1. - f_0
  f = np.hstack([f_0, f_1])
  return f, y
{% endhighlight %}

下图显示了交叉熵 $$L$$ 与参数 $$a$$ 之间的曲线图， $$N$$ 取值 10e4。
![L-vs-a](/images/crossentropy_beta_a.png)

可以看出随着 `f` 不断的接近 `y` ($$a$$ 的不断增大)，$$L$$ 的值不断下降。

[one-hot-wiki]: https://en.wikipedia.org/wiki/One-hot
