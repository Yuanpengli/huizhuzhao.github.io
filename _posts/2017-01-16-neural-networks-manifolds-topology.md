---
layout: post
title:  神经网络，流形和拓扑
date:   2017-01-16
---
A chinese version of blog [Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) (by [Christopher Olah](http://colah.github.io/about.html))

以下内容直接翻译自博客 [Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) (作者 [Christopher Olah](http://colah.github.io/about.html))

近来，深度神经网络引起人们极大的热情和兴趣，这源于深度神经网络模型在计算机视觉领域所取得的突破。

然而，对于这些模型仍有很多未解决的问题。其中一个便是我们仍不能完全理解神经网络模型的工作原理。比如，当我们把模型训练的恰到好处，该模型
便能够取得高质量的结果，但我们很难解释清楚模型是如何做到这一点的；另外，在模型训练的不是那么恰好的情况下，我们又不知道到底是哪里出了问题。

尽管从基本面上去理解神经网络的工作原理是一件具有挑战性的事情，但是对于理解低维深度神经网络来说是一件相对容易的事情，这里低维是指神经网络模型中每一层仅有少量个数的神经元。事实上，我们可以通过创建可视化来彻底的理解
低维网络模型的行为和训练过程。并从这个角度，对神经网络的行为产生更深的直觉，并且发现神经网络和数学领域的拓扑之间的联系。

在此联系中，我们会发现许多有趣的事情，包括一个神经网络模型的复杂度至少需要达到怎样的下限值，才能实现对特定的数据集进行正确分类的能力。

### 一个简单的例子

我们从一个简单的数据集开始，该数据集是平面上的两条曲线数据。神经网络将对数据集中的点进行分类--即某个数据点属于两条曲线中的哪一条。

![olah_two_curves](/images/olah_two_curves.png)

想要可视化神经网络的行为，一种直接的方式便是去查看它是怎样分类每一个可能的数据点，这种方式同样适用于其他分类算法。

我们先以最简单的一类神经网络模型为例，该网络仅有一个输入层和一个输出层。该网络仅是简单的用一条直线对两类数据点进行区分。如下图所示

![olah_two_curves_2](/images/olah_two_curves_2.png)

这类简单的神经网络并没有太大的用途。深度神经网络在输入/输出层之间通常具有多个隐藏层。下图是只有一个隐藏层的情况下的神经网络

![nn_one_hidden_layer](/images/nn_one_hidden_layer.png)

现在，上面的神经网络将如何对数据集进行分类呢？它是以一种更加复杂的曲线来对两类数据点进行分类

![nn_one_hidden_layer_2](/images/nn_one_hidden_layer_2.png)

神经网络中的每一层将对输入的数据进行变换，生成新的特征表示(representation)。我们可以通过观察每一层的特征表示来查看网络是如何进行分类的。另外，当我们观测到最后一层特征表示的时候，我们将发现网络仅仅
使用一条线(在高维情形中则是一个超平面)对数据点进行分类。

在之前的可视化中，我们观察的是数据集的“原始”特征表示，这等同于观察输入层的数据。现在我们来观察数据经过第一次变换后的情况，也就是隐藏层的特征表示。

下图中的每一维对应于隐藏层中一个神经元的激活情况。

![nn_one_hidden_layer_3](/images/nn_one_hidden_layer_3.png)

### 网络层的连续可视化

上一节中，我们通过观察每一层的特征表示来试着理解该网络模型。至此，我们得到了一个特征表示的列表(列表中每个特征表示对应一个网络层)。

然而更重要的是如何理解从一个特征表示（前一层）得到另一个特征表示（后一层）。幸好，神经网络层具有很好的属性，使得这个过程变得非常容易。

对于神经网络模型，可选的层有很多种类型，这里我们着重讨论 **tanh** 层。一个 $$tanh(Wx+b)$$ 层包括如下几步

 1. 通过权重矩阵 $$W$$ 进行线性变换 
 2. 通过向量 $$b$$ 进行平移
 3. 逐点进行 $$tanh$$ 函数变换

其他类型的网络层所做的工作也类似，即仿射变换，及随后逐点应用单调激活函数。

我们通过上面的步骤来理解更为复杂的神经网络模型。比如，下面的网络将两条轻微交缠在一起的螺旋线进行分类，网络中使用了四个隐藏层。随着时间，
我们看到模型将“原始”数据变换到它所学习到的更高维度的特征表示，从而使得一开始纠缠在一起的螺旋线，在高维空间里变得线性可分。

另一方面，下面的网络模型虽然也包含多个隐藏层，但并不能把两条纠缠更为紧密的螺旋线成功的分类。

这里需要点明上面的例子之所以看起来很有挑战性，是因为我们使用了低维类型的神经网络模型。如果使用更宽(wider)的网络模型，也就是隐藏层中包含
更多的神经元，上面的问题解决起来就会很简单了。